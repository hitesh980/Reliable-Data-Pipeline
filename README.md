# Reliable Data Pipeline – Orders

This repository demonstrates a small, production-minded batch pipeline:
- Synthetic CSV data is generated by a Python script.
- Airflow runs the ingestion task to load CSV data into Postgres (`raw_orders`).
- dbt builds staging models on top of the raw table (`stg_orders`) and runs tests.
This README contains quick setup, run and verification instructions so you can
get the pipeline working locally using Docker.

Quick architecture
------------------
CSV (synthetic) → Airflow (PythonOperator) → Postgres (raw_orders) → dbt (stg_orders, marts) → dbt tests

Prerequisites
-------------
- Docker & Docker Compose installed
- A POSIX shell (macOS/Linux). Commands below assume you run them from the
  project root: the folder that contains `docker-compose.yml`.

Start the stack
---------------
Build and start the services (Airflow + Postgres):

```bash
docker compose up -d --build
```

Wait ~20–30s for containers to initialize.
Airflow UI
----------
- Open: http://localhost:8080
- Default user created by the compose setup: `admin` / `admin`

Run the pipeline
----------------
1. Trigger the DAG `reliable_orders_dag` from the Airflow UI OR trigger it from
  the host:

```bash
docker exec reliable_airflow airflow dags trigger reliable_orders_dag
```

2. The DAG will:
  - generate synthetic CSV under `/opt/airflow/data/raw/orders.csv` (mounted to host `./data/raw`),
  - load the CSV into Postgres table `raw_orders`,
  - run `dbt run` to compile & build `stg_orders`,
  - run `dbt test` to execute data tests.

Run dbt manually (debug & tests)
-------------------------------
To run dbt commands from inside the Airflow container:

```bash
docker exec -it reliable_airflow bash
cd /opt/airflow/dbt
dbt deps       # install package dependencies
dbt debug      # verify profiles and connections
dbt run        # build models
dbt test       # run tests

Files & locations you’ll care about
----------------------------------
- `airflow/plugins/generate_orders.py` — generator used by Airflow to create CSV
- `airflow/plugins/load_orders.py` — loader that writes to Postgres
- `airflow/dags/reliable_orders_dag.py` — DAG wiring load → dbt run → dbt test
- `airflow/dbt/` — dbt project mounted into the Airflow container at `/opt/airflow/dbt`
- `data/raw/orders.csv` — generated CSV on the host (mounted into container)

Common troubleshooting
----------------------
- dbt packages not installed: run `dbt deps` inside the container (see above).
- DB connection failures: confirm `airflow` and `postgres` containers are running
  (`docker ps`) and profiles file is mounted to `/home/airflow/.dbt/profiles.yml`.
- If dbt complains about missing `raw_orders` model: ensure the ingestion task
  in Airflow has run and created the `raw_orders` table in Postgres.

Verification commands (host)
---------------------------
Check containers:
```bash
docker ps --filter "name=reliable"
```

Check raw rows in Postgres:
```bash
docker exec -it reliable_postgres psql -U data_user -d analytics -c "SELECT COUNT(*) FROM raw_orders;"
```

Check dbt status inside Airflow container:
```bash
docker exec reliable_airflow bash -lc "cd /opt/airflow/dbt && dbt debug && dbt run && dbt test"
```

Notes
-----
- This repository intentionally keeps ingestion (Airflow) and transformation
  (dbt) separated; dbt is run from inside the Airflow container against the
  same Postgres service running in Docker.
- The project uses `dbt_utils` for a small test; if tests fail, run `dbt deps`.

If you want, I can also add a short `Makefile` with convenience targets, or
help add automated CI checks that run `dbt run` and `dbt test`.

Enjoy the pipeline!
# Reliable Data Pipeline – Orders

## Overview
This project demonstrates a production-style reliable batch data pipeline using
Airflow, Postgres, and dbt.

Synthetic order data is generated, ingested into Postgres, transformed using dbt,
and validated using data quality tests.

## Architecture
CSV (Synthetic)  
→ Airflow (PythonOperator)  
→ Postgres (raw_orders)  
→ dbt staging (stg_orders)  
→ dbt marts (fct_orders)  
→ dbt tests (fail-fast)

## Reliability Features
- Idempotent loads
- Schema validation before load
- dbt tests for:
  - not null constraints
  - uniqueness
  - value ranges
- Test-after-run pattern in Airflow
- Clear separation of ingestion vs transformation

## Technologies
- Apache Airflow
- PostgreSQL
- dbt
- Docker
- Python (pandas, psycopg2, Faker)

## How to Run
1. docker compose up -d
2. Open Airflow UI
3. Trigger `reliable_orders_dag`
